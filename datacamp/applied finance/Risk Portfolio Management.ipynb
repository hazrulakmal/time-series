{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Profotlio Management\n",
    "identify factor which are driving fortfolio  returns\n",
    "\n",
    "Investment Risk\n",
    "- measure of uncertainty of future returns (dispersion or variance of financial returns)\n",
    "\n",
    "Percentage Change and log percentage change\n",
    "\n",
    "The Adjusted Close column is the most important. It is normalized for stock splits, dividends, and other corporate actions, and is a true reflection of the return of the stock over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Investment Risk & Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the csv file and parse dates\n",
    "StockPrices = pd.read_csv(fpath_csv, parse_dates=['Date'])\n",
    "\n",
    "# Ensure the prices are sorted by Date\n",
    "StockPrices = StockPrices.sort_values(by='Date')\n",
    "\n",
    "# Calculate the daily returns of the adjusted close price\n",
    "StockPrices['Returns'] = StockPrices['Adjusted'].pct_change()\n",
    "\n",
    "# Plot the returns column over time\n",
    "StockPrices['Returns'].plot()\n",
    "plt.show()\n",
    "\n",
    "# Convert the decimal returns into percentage returns\n",
    "percent_return = StockPrices['Returns']*100\n",
    "\n",
    "# Drop the missing values\n",
    "returns_plot = percent_return.dropna()\n",
    "\n",
    "# Plot the returns histogram\n",
    "plt.hist(returns_plot, bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Deviation (voltality)'\n",
    "- an investement with higer standard deviation (voltality) is viewed as higher risk investment \n",
    "- scalling voltality, \n",
    "    - $\\sigma_{annual}$  = $ \\sigma_{daily} \\times \\sqrt {252}$,  \n",
    "    - $\\sigma_{monthly}$  = $ \\sigma_{daily} \\times  \\sqrt {21}$\n",
    "\n",
    "Average annualised return = $(1+\\mu)^{252}-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the average daily return of the stock\n",
    "mean_return_daily = np.mean((StockPrices['Returns']))\n",
    "print(mean_return_daily)\n",
    "\n",
    "# Calculate the implied annualized average return\n",
    "mean_return_annualized = ((1+mean_return_daily)**252)-1\n",
    "print(mean_return_annualized)\n",
    "\n",
    "# Calculate the standard deviation of daily return of the stock\n",
    "sigma_daily = np.std(StockPrices['Returns'])\n",
    "print(sigma_daily)\n",
    "\n",
    "# Calculate the daily variance\n",
    "variance_daily = sigma_daily**2\n",
    "print(variance_daily)\n",
    "\n",
    "# Annualize the standard deviation\n",
    "sigma_annualized = sigma_daily*np.sqrt(252)\n",
    "print(sigma_annualized)\n",
    "\n",
    "# Calculate the annualized variance\n",
    "variance_annualized = sigma_annualized**2\n",
    "print(variance_annualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- skew - wether it's left-leaning(positive) or right-leaning(negative). in finance we prefer left-leaning positive skew because the loss are predictable with long right distribution for high returns.\n",
    "- Kurtosis is a measure of how thick the tail of a distribution \n",
    "- most financial returns are leptokratic (when  a distribution has a postive excess kurtosis (higher than 3)) normal distribution has a kurtosis of 3\n",
    "- excess kurtosis = kurtosis - 3 indicates higher risk (outliers, the far right & the far left, are more common - relatively higher probability than a normal distribution)\n",
    "\n",
    "Testing for normality - Shapiro-Wilk test (null hypothesis - the ditribution is normal) so if p-values is less thann -0.05 we can safely assume that the distibution is non-normal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import skew from scipy.stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Drop the missing values\n",
    "clean_returns = StockPrices[\"Returns\"].dropna()\n",
    "\n",
    "# Calculate the third moment (skewness) of the returns distribution\n",
    "returns_skewness = skew(clean_returns)\n",
    "print(returns_skewness)\n",
    "\n",
    "# Import kurtosis from scipy.stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# Calculate the excess kurtosis of the returns distribution\n",
    "excess_kurtosis = kurtosis(clean_returns)\n",
    "print(excess_kurtosis)\n",
    "\n",
    "# Derive the true fourth moment of the returns distribution\n",
    "fourth_moment = excess_kurtosis + 3\n",
    "print(fourth_moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Composition & Backstasting\n",
    "\n",
    "Portfolio refers to a bundle of different stocks each with different weight.\n",
    "- $R_p $ , Portfolio return\n",
    "- $R_{a_n} $, Asset n Return\n",
    "- $w_{a_n}$, Weight for asset n\n",
    "$$ R_p = R_{a_1}w_{a_1} + R_{a_2}w_{a_2} + \\dots + R_{a_n}w_{a_n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish defining the portfolio weights as a numpy array\n",
    "portfolio_weights = np.array([0.12, 0.15, 0.08, 0.05, 0.09, 0.10, 0.11, 0.14, 0.16])\n",
    "\n",
    "# Calculate the weighted stock returns\n",
    "WeightedReturns = StockReturns.mul(portfolio_weights, axis=1)\n",
    "\n",
    "# Calculate the portfolio returns\n",
    "StockReturns['Portfolio'] = WeightedReturns.sum(axis=1)\n",
    "\n",
    "# Plot the cumulative portfolio returns over time\n",
    "CumulativeReturns = ((1+StockReturns[\"Portfolio\"]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()\n",
    "\n",
    "# How many stocks are in your portfolio?\n",
    "numstocks = 9\n",
    "\n",
    "# Create an array of equal weights across all assets\n",
    "portfolio_weights_ew = np.repeat(1/numstocks, numstocks)\n",
    "\n",
    "# Calculate the equally-weighted portfolio returns\n",
    "StockReturns['Portfolio_EW'] = StockReturns.iloc[:, :9].mul(portfolio_weights_ew, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW'])\n",
    "\n",
    "# Create an array of market capitalizations (in billions)\n",
    "market_capitalizations = np.array([601.51, 469.25, 349.5, 310.48, 299.77, 356.94, 268.88, 331.57, 246.09])\n",
    "\n",
    "# Calculate the market cap weights\n",
    "mcap_weights = market_capitalizations/sum(market_capitalizations)\n",
    "\n",
    "# Calculate the market cap weighted portfolio returns\n",
    "StockReturns['Portfolio_MCap'] = StockReturns.iloc[:, 0:9].mul(mcap_weights, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW', 'Portfolio_MCap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation & Covariance \n",
    "\n",
    "- Finding a linear correlation between two stocks\n",
    "\n",
    "$ \\mathrm{Cov}(A,B) = Var(A) + Var(B) + 2Var(A)Var(B)$ Covariance\n",
    "When dealing with multiple stocks, we can deal with covariance matrix \n",
    "\n",
    "Protfolio standart deviation\n",
    "- $ \\sigma_{Portfolio}^2 $  Portfolio variance\n",
    "- $ w $ weights vector\n",
    "- $ \\sum $ covariance matrix \n",
    "$$ \\sigma_{Portfolio} = \\sqrt{w_T \\cdot  \\sum  \\cdot w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = StockReturns.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Import seaborn as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=True,\n",
    "            cmap=\"YlGnBu\", \n",
    "            linewidths=0.3,\n",
    "            annot_kws={\"size\": 8})\n",
    "\n",
    "# Plot aesthetics\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0) \n",
    "plt.show()\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_mat = StockReturns.cov()\n",
    "\n",
    "# Annualize the co-variance matrix\n",
    "cov_mat_annual = cov_mat*(252) # no need to sqrt 252 because it's a variance not standard deviation\n",
    "\n",
    "# Print the annualized co-variance matrix\n",
    "print(cov_mat_annual)\n",
    "\n",
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the portfolio standard deviation\n",
    "portfolio_volatility = np.sqrt(np.dot(portfolio_weights.T, np.dot(cov_mat_annual, portfolio_weights)))\n",
    "print(portfolio_volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markowitz Portfolio\n",
    "\n",
    "sharpe ratio is a measure of risk adjusted return\n",
    "\n",
    "$$ S = \\frac {R_{\\alpha} - r_{f}}{\\sigma_{\\alpha}}$$\n",
    "- S, sharpe ratio\n",
    "- $ R_{\\alpha} $ Asset return\n",
    "- $ r_{f} $ risk-free Asset return\n",
    "- $ \\sigma_{\\alpha} $ Asset volatilty\n",
    "\n",
    "how much return an investor can expect for each incremental unit of risk and can be used to compare different portfolio with different risk\n",
    "\n",
    "The *maximum Sharpe ratio*, or *MSR portfolio*, which lies at the apex of the efficient frontier, can be constructed by looking for the portfolio with the highest Sharpe ratio.\n",
    "\n",
    "Unfortunately, the MSR portfolio is often quite erratic. Even though the portfolio had a high historical Sharpe ratio, it doesn't guarantee that the portfolio will have a good Sharpe ratio moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk free rate\n",
    "risk_free = 0\n",
    "\n",
    "# Calculate the Sharpe Ratio for each asset\n",
    "#annualized rate of return and annualized standard deviation\n",
    "RandomPortfolios['Sharpe'] = (RandomPortfolios[\"Returns\"] - risk_free)/RandomPortfolios[\"Volatility\"]\n",
    "\n",
    "\n",
    "# Print the range of Sharpe ratios\n",
    "print(RandomPortfolios['Sharpe'].describe()[['min', 'max']])\n",
    "\n",
    "RandomPortfolios.head()\n",
    "\n",
    "# Sort the portfolios by Sharpe ratio\n",
    "sorted_portfolios = RandomPortfolios.sort_values(by=['Sharpe'], ascending=False)\n",
    "\n",
    "# Extract the corresponding weights\n",
    "MSR_weights = sorted_portfolios.iloc[0, 0:numstocks]\n",
    "\n",
    "# Cast the MSR weights as a numpy array\n",
    "MSR_weights_array = np.array(MSR_weights)\n",
    "\n",
    "# Calculate the MSR portfolio returns\n",
    "StockReturns['Portfolio_MSR'] = StockReturns.iloc[:, 0:numstocks].mul(MSR_weights_array, axis=1).sum(axis=1)\n",
    "\n",
    "# Plot the cumulative returns\n",
    "cumulative_returns_plot(['Portfolio_EW', 'Portfolio_MCap', 'Portfolio_MSR'])\n",
    "\n",
    "\n",
    "# Sort the portfolios by volatility\n",
    "sorted_portfolios = RandomPortfolios.sort_values(by=['Volatility'], ascending=True)\n",
    "\n",
    "# Extract the corresponding weights\n",
    "GMV_weights = sorted_portfolios.iloc[0, 0:numstocks]\n",
    "\n",
    "# Cast the GMV weights as a numpy array\n",
    "GMV_weights_array = np.array(GMV_weights)\n",
    "\n",
    "# Calculate the GMV portfolio returns\n",
    "StockReturns['Portfolio_GMV'] = StockReturns.iloc[:, 0:numstocks].mul(GMV_weights_array, axis=1).sum(axis=1)\n",
    "\n",
    "# Plot the cumulative returns\n",
    "cumulative_returns_plot(['Portfolio_EW', 'Portfolio_MCap', 'Portfolio_MSR', 'Portfolio_GMV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capital Asset Pricing Model (CAPM)\n",
    "\n",
    "excess rate = return rate - risk-free return \n",
    "\n",
    "$$ E(R_p) - RF = \\beta_p(E(R_b)- RF)$$\n",
    "\n",
    "- $ R_p - RF$ , excess rate of expected rate of return of portfolio P\n",
    "- $ R_b $ RF , excess rate of expected rate of return of the broad market portfolio B\n",
    "- the reginol risk free rate\n",
    "\n",
    "- $ \\beta_p $ beta p or exposure to the broad market portfolio B\n",
    "\n",
    "the higher the beta, the higher the exposure to the broad market portfolio B\n",
    "\n",
    "Calculate historical beta using co-variance \n",
    "\n",
    "$$ \\beta_p = \\frac{Cov(R_p, R_b)}{Var(R_b)}$$\n",
    "\n",
    "- $Cov(R_p, R_b)$ co-variance between portfolio P  and  bencmark maket index b\n",
    "- $ Var(R_b)$ the variance of the benchmark market index B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate excess portfolio returns\n",
    "FamaFrenchData['Portfolio_Excess'] = FamaFrenchData[\"Portfolio\"] - FamaFrenchData[\"RF\"]\n",
    "\n",
    "# Plot returns vs excess returns\n",
    "CumulativeReturns = ((1+FamaFrenchData[['Portfolio','Portfolio_Excess']]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()\n",
    "\n",
    "FamaFrenchData.head()\n",
    "\n",
    "# Calculate the co-variance matrix between Portfolio_Excess and Market_Excess\n",
    "covariance_matrix = FamaFrenchData[['Portfolio_Excess', 'Market_Excess']].cov()\n",
    "\n",
    "# Extract the co-variance co-efficient\n",
    "covariance_coefficient = covariance_matrix.iloc[0, 1]\n",
    "print(covariance_coefficient)\n",
    "\n",
    "# Calculate the benchmark variance\n",
    "benchmark_variance = FamaFrenchData['Market_Excess'].var()\n",
    "print(benchmark_variance)\n",
    "\n",
    "# Calculating the portfolio market beta\n",
    "portfolio_beta = covariance_coefficient/benchmark_variance\n",
    "print(portfolio_beta)\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Define the regression formula\n",
    "CAPM_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess', data=FamaFrenchData)\n",
    "\n",
    "# Print adjusted r-squared of the fitted regression\n",
    "CAPM_fit = CAPM_model.fit()\n",
    "print(CAPM_fit.rsquared_adj)\n",
    "\n",
    "# Extract the beta\n",
    "regression_beta = CAPM_fit.params[\"Market_Excess\"]\n",
    "print(regression_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## portfolio interpretaion\n",
    "Your portfolio beta is 0.9738. You can think of market beta as a measure of your exposure to the broad stock market. For every 1.0% rise (or fall) in the market, you can expect your portfolio to rise (fall) roughly 0.97%.\n",
    "\n",
    "The adjusted r-squared is 0.7943. A high adjusted r-squared (close to 1) means that the majority of your portfolio's movements can be explained by the factors in your model.\n",
    "\n",
    "\n",
    "The Fama-French 3 Factors model\n",
    "$$ R_p  = RF+ \\beta_b(R_b- RF) + b_{SMB} \\cdot SMB + b_{HML} \\cdot HML + \\alpha$$\n",
    "\n",
    "- SMB, small minus big\n",
    "- HML high minus low factor\n",
    "- b, beta for both SMB and HML are the exposure of the portolio to the HML factor and SMB factor\n",
    "- alpha is an error term. in the efficient market, alpha is equivalent to zero\n",
    "\n",
    "\n",
    "The HML factor is constructed by calculating the return of growth stocks, or stocks with high valuations, versus the return of value stocks.\n",
    "The SMB factor is constructed by calculating the return of small-cap stocks, or stocks with small market capitalizations, versus the return of large-cap stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "FamaFrench_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess + SMB + HML ', data=FamaFrenchData)\n",
    "\n",
    "# Fit the regression\n",
    "FamaFrench_fit = FamaFrench_model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = FamaFrench_fit.rsquared_adj\n",
    "print(regression_adj_rsq)\n",
    "\n",
    "# Extract the p-value of the SMB factor\n",
    "smb_pval = FamaFrench_fit.pvalues[\"SMB\"]\n",
    "\n",
    "# If the p-value is significant, print significant\n",
    "if smb_pval < 0.05:\n",
    "    significant_msg = 'significant'\n",
    "else:\n",
    "    significant_msg = 'not significant'\n",
    "\n",
    "# Print the SMB coefficient\n",
    "smb_coeff = FamaFrench_fit.params[\"SMB\"]\n",
    "print(\"The SMB coefficient is \", smb_coeff, \" and is \", significant_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Tail Risk\n",
    "\n",
    "### Historical Drawdown\n",
    "the percentage loss from the highest cumulative historical point \n",
    "\n",
    "$$ D = \\frac{r_t}{RM} - 1$$\n",
    "\n",
    "- D, drawdown\n",
    "- $ r_t $ cumulative return at time t\n",
    "- RM Running Maximum \n",
    "\n",
    "### Value at Risk\n",
    "there is quantile attached to it. for example if 95-th percentile is choosen. at the worst 5% of the trading days, the investement will loss more than x percentage/b with 95 cenrtainty, the loss will not excedd x percentage in a given day based on historical data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the running maximum\n",
    "running_max = np.maximum.accumulate(cum_rets)\n",
    "\n",
    "# Ensure the value never drops below 1\n",
    "running_max[running_max < 1] = 1\n",
    "\n",
    "# Calculate the percentage drawdown\n",
    "drawdown = (cum_rets)/running_max - 1\n",
    "\n",
    "# Plot the results\n",
    "drawdown.plot()\n",
    "plt.show()\n",
    "\n",
    "# Calculate historical VaR(95)\n",
    "var_95 = np.percentile(StockReturns_perc, 5)\n",
    "print(var_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = StockReturns_perc.sort_values()\n",
    "\n",
    "# Plot the probability of each sorted return quantile\n",
    "plt.hist(sorted_rets, density=True, stacked=True)\n",
    "\n",
    "# Denote the VaR 95 quantile\n",
    "plt.axvline(x=var_95, color='r', linestyle='-', label=\"VaR 95: {0:.2f}%\".format(var_95))\n",
    "plt.show()\n",
    "\n",
    "# Historical CVaR 95\n",
    "cvar_95 = StockReturns_perc[StockReturns_perc <  var_95].mean()\n",
    "print(cvar_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = sorted(StockReturns_perc)\n",
    "\n",
    "# Plot the probability of each return quantile\n",
    "plt.hist(sorted_rets, density=True, stacked=True)\n",
    "\n",
    "# Denote the VaR 95 and CVaR 95 quantiles\n",
    "plt.axvline(x=var_95, color=\"r\", linestyle=\"-\", label='VaR 95: {0:.2f}%'.format(var_95))\n",
    "plt.axvline(x=cvar_95, color='b', linestyle='-', label='CVaR 95: {0:.2f}%'.format(cvar_95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value at Risk can also be computed parametrically using a method known as variance/co-variance VaR. This method allows you to simulate a range of possibilities based on historical return distribution properties rather than actual return values. You can calculate the parametric VaR(90) using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import norm from scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Estimate the average daily return\n",
    "mu = np.mean((StockReturns))\n",
    "\n",
    "# Estimate the daily volatility\n",
    "vol = np.std((StockReturns))\n",
    "\n",
    "# Set the VaR confidence level\n",
    "confidence_level = 0.05\n",
    "\n",
    "# Calculate Parametric VaR\n",
    "var_95 = norm.ppf(confidence_level, mu, vol)\n",
    "print('Mean: ', str(mu), '\\nVolatility: ', str(vol), '\\nVaR(95): ', str(var_95))\n",
    "\n",
    "# Aggregate forecasted VaR\n",
    "forecasted_values = np.empty([100, 2])\n",
    "\n",
    "# Loop through each forecast period\n",
    "for i in range(100):\n",
    "    # Save the time horizon i\n",
    "    forecasted_values[i, 0] = i\n",
    "    # Save the forecasted VaR 95\n",
    "    forecasted_values[i, 1] = var_95*np.sqrt(i+1)\n",
    "    \n",
    "# Plot the results\n",
    "plot_var_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monte carlo simulation\n",
    "\n",
    "to create thousands of possible casses. we can analyse each one of them just like we did before - looking at VAr & CVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the simulation parameters\n",
    "mu = np.mean(StockReturns)\n",
    "vol = np.std(StockReturns)\n",
    "T = 252\n",
    "S0 = 10\n",
    "\n",
    "# Add one to the random returns\n",
    "rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "\n",
    "# Forecasted random walk\n",
    "forecasted_values = rand_rets.cumprod() * S0\n",
    "\n",
    "# Plot the random walk\n",
    "plt.plot(range(0, T), forecasted_values)\n",
    "plt.show()\n",
    "\n",
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the random returns\n",
    "    rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "    \n",
    "    # Create the Monte carlo path\n",
    "    forecasted_values = S0*(rand_rets).cumprod()\n",
    "    \n",
    "    # Plot the Monte Carlo path\n",
    "    plt.plot(range(T), forecasted_values)\n",
    "\n",
    "# Show the simulations\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Aggregate the returns\n",
    "sim_returns = []\n",
    "\n",
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the Random Walk\n",
    "    rand_rets = np.random.normal(mu, vol, T)\n",
    "    \n",
    "    # Save the results\n",
    "    sim_returns.append(rand_rets)\n",
    "\n",
    "# Calculate the VaR(99)\n",
    "var_99 = np.percentile(sim_returns, 1)\n",
    "print(\"Parametric VaR(99): \", round(100*var_99, 2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Moments and Distibutions\n",
    "- Portfolio composition\n",
    "- Correlation & Covariance\n",
    "- Markowitz optimisation\n",
    "- Beta & CAPM\n",
    "- FAMA French Factor Modelling\n",
    "- ALpha - term that measures the efficient market\n",
    "- Var & CVar\n",
    "- Monte Carlo Simulation\n",
    "\n",
    "\n",
    "## Covariance Matrix\n",
    "\n",
    "- **Diagonal entries** are variance of each individual asset\n",
    "- **off-diagonal entries** are covariance between two different asset depending on which cell we are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit (windows store)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Users/Hazrul Akmal/AppData/Local/Microsoft/WindowsApps/python3.9.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Calculate the 30-day rolling window of portfolio returns\n",
    "returns_windowed = portfolio_returns.rolling(20)\n",
    "\n",
    "# Compute the annualized volatility series\n",
    "volatility_series = returns_windowed.std()*np.sqrt(252)\n",
    "\n",
    "# Plot the portfolio volatility\n",
    "volatility_series.plot().set_ylabel(\"Annualized Volatility, 30-day Window\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk factors and financial crisis\n",
    "\n",
    "### Volatility \n",
    "- measure of dispersion of returns around expected value\n",
    "### Expected return\n",
    "- sample mean in a time-series context\n",
    "\n",
    "What drives expected returns and volatility?\n",
    "- the drives are known as risk factors: events and variables driving portfolio returns and volatility\n",
    "\n",
    "#### Systematic Risk\n",
    "risk factor(s) affecting volatility of all portfolio assets\n",
    "one example is market risk; systematic risk from general financial market movements \n",
    "\n",
    "example of financial systemic risk\n",
    "- inflation, interest rate changes, economic climate changes\n",
    "\n",
    "#### Idiosyncratic Risk\n",
    "- risk spesific to a particular class/asset class\n",
    "example- bond asset : issuer risk and default, firm/sector characterisation: market cap, sector shocks, book-to-market value ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily returns to quarterly average returns\n",
    "returns_q = returns.resample('Q').mean()\n",
    "\n",
    "# Examine the beginning of the quarterly series\n",
    "print(returns_q.head())\n",
    "\n",
    "# Now convert daily returns to weekly minimum returns\n",
    "returns_w = returns.resample('W').min()\n",
    "\n",
    "# Examine the beginning of the weekly series\n",
    "print(returns_w.head)\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(port_q_mean, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Portfolio Theory\n",
    "\n",
    "- aim is to always maximise returns given a risk level\n",
    "- one way to do this by adjusting the weights of each asset in portfolio. the method is called efficient portfolio theory - portfolio with weights generating the highest expected return given a level of risk.\n",
    "- weights and expected returns can be visualised on efficient frontier plots \n",
    "\n",
    "Portfolio optimization relies upon an *unbiased* and *efficient* estimate of asset covariance. Although sample covariance is unbiased, it is not efficient--extreme events tend to be overweighted.\n",
    "\n",
    "One approach to alleviate this is through \"covariance shrinkage\", where large errors are reduced ('shrunk') to improve efficiency. In this exercise, you'll use pypfopt.risk_models's CovarianceShrinkage object to transform sample covariance into an efficient estimate. The textbook error shrinkage method, .ledoit_wolf(), is a method of this object.\n",
    "\n",
    "Asset prices are available in your workspace. Note that although the CovarianceShrinkage object takes prices as input, it actually calculates the covariance matrix of asset returns, not prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment portfolio price data into the price variable.\n",
    "prices = pd.read_csv(\"portfolio.csv\")\n",
    "\n",
    "# Convert the 'Date' column to a datetime index\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], format='%d/%m/%Y')\n",
    "prices.set_index(['Date'], inplace = True)\n",
    "\n",
    "# Import the mean_historical_return method\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "\n",
    "# Compute the annualized average historical return\n",
    "mean_returns = expected_returns(prices, frequency = 252)\n",
    "\n",
    "# Plot the annualized average historical return\n",
    "plt.plot(mean_returns, linestyle = 'None', marker = 'o')\n",
    "plt.show()\n",
    "\n",
    "# Import the CovarianceShrinkage object\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "\n",
    "# Create the CovarianceShrinkage instance variable\n",
    "cs = CovarianceShrinkage(prices)\n",
    "\n",
    "# Compute the sample covariance matrix of returns\n",
    "sample_cov = prices.pct_change().cov() * 252\n",
    "\n",
    "# Compute the efficient covariance matrix of returns\n",
    "e_cov = cs.ledoit_wolf()\n",
    "\n",
    "# Display both the sample covariance_matrix and the efficient e_cov estimate\n",
    "print(\"Sample Covariance Matrix\\n\", sample_cov, \"\\n\")\n",
    "print(\"Efficient Covariance Matrix\\n\", e_cov, \"\\n\")\n",
    "\n",
    "# Create a dictionary of time periods (or 'epochs')\n",
    "epochs = { 'before' : {'start': '1-1-2005', 'end': '31-12-2006'},\n",
    "           'during' : {'start': '1-1-2007', 'end': '31-12-2008'},\n",
    "           'after'  : {'start': '1-1-2009', 'end': '31-12-2010'}\n",
    "         }\n",
    "\n",
    "# Compute the efficient covariance for each epoch\n",
    "e_cov = {}\n",
    "for x in epochs.keys():\n",
    "  sub_price = prices.loc[epochs[x]['start']:epochs[x]['end']]\n",
    "  e_cov[x] = CovarianceShrinkage(sub_price).ledoit_wolf()\n",
    "\n",
    "# Display the efficient covariance matrices for all epochs\n",
    "print(\"Efficient Covariance Matrices\\n\", e_cov)\n",
    "\n",
    "# Initialize the Crtical Line Algorithm object\n",
    "efficient_portfolio_during = CLA(returns_during, ecov_during)\n",
    "\n",
    "# Find the minimum volatility portfolio weights and display them - $min_volatility is a method\n",
    "print(efficient_portfolio_during.min_volatility())\n",
    "\n",
    "# Compute the efficient frontier\n",
    "(ret, vol, weights) = efficient_portfolio_during.efficient_frontier() #efficint_frontier()\n",
    "\n",
    "# Add the frontier to the plot showing the 'before' and 'after' frontiers\n",
    "plt.scatter(vol, ret, s = 4, c = 'g', marker = '.', label = 'During')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding between options\n",
    "- wether or not to pay extra to get refunable booking made if anything happes.\n",
    "\n",
    "1. chance of negative shock (probability of loss) \n",
    "2. loss associated with shocks - Var, CVar'\n",
    "3. Desire to avoid shock (personal feeling )- risk tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Student's t-distribution\n",
    "from scipy.stats import t\n",
    "\n",
    "# Create rolling window parameter list\n",
    "mu = losses.rolling(30).mean()\n",
    "sigma = losses.rolling(30).std()\n",
    "rolling_parameters = [(29, mu[i], s) for i,s in enumerate(sigma)]\n",
    "\n",
    "# Compute the 99% VaR array using the rolling window parameters\n",
    "VaR_99 = np.array( [ t.ppf(0.99, *params) \n",
    "                    for params in  rolling_parameters] )\n",
    "\n",
    "# Plot the minimum risk exposure over the 2005-2010 time period\n",
    "plt.plot(losses.index, 0.01 * VaR_99 * 100000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9c71197a12097caaa94dbd6926b7c26d52a7c73c772ca71a63d19ef0206318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
